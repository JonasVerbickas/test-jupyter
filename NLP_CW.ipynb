{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonasVerbickas/test-jupyter/blob/main/NLP_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import time\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import Counter\n",
        "from collections import UserDict\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "BTTGCTeE65K-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm not sure how you will access the corpus on your side.\n",
        "Personally, I've uploaded it to my Google drive."
      ],
      "metadata": {
        "id": "tF3YIutv5fK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "A59xTdZA0OzU",
        "outputId": "45c9745c-d6b0-4f0b-c4d3-796d4278d74d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter = nltk.PorterStemmer()\n",
        "porter.stem(\"home\")"
      ],
      "metadata": {
        "id": "9Kh-pY5D8H1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c779a2f-6205-4ce9-e21f-1136b99b5b13"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'home'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download additional NLTK resources\n",
        "These are required for tokenization and stop-word removal"
      ],
      "metadata": {
        "id": "tSyqexBZ3npQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# I convert it into a set, because it results in faster `in` look-ups\n",
        "STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
        "# convert to set() for faster `in` checks\n",
        "STOP_WORDS = set(STOP_WORDS)\n",
        "# for some reason would is missing by default\n",
        "STOP_WORDS.add('would')\n",
        "# t an `episode` most common term is the possesive `'s`\n",
        "# i'd argue that it should be removed, because multi-word terms that contain possesives\n",
        "# should be findable without `'s` e.g. \"moe tavern\" should match with \"Moe's tavern\", \"bart treehouse\" should match \"Bart's treehouse\"\n",
        "# this will lead to some \n",
        "STOP_WORDS.add(\"'s\")\n",
        "# this is supposed to be used for tokenized 'won't' however it eliminates `won` as in winning as well\n",
        "# I will expand all contractions and this stop-word will not be needed\n",
        "STOP_WORDS.remove('won')\n",
        "STOP_WORDS"
      ],
      "metadata": {
        "id": "fXosFc5y2vyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "FIZUgB4Qltb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Initialize helper classes\n",
        "I could've coded everything using tuples/lists instead of these helper classes.\n",
        "\n",
        "However, I feel like these make the code more verbose and remove ambiguity that comes from giving positions in a tuple meaning."
      ],
      "metadata": {
        "id": "1aYg16m74cxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StringWithDocId:\n",
        "  \"\"\"\n",
        "  Used in InvertedIndex.read_data method to create a list of document contents with doc_ids \n",
        "  \"\"\" \n",
        "  def __init__(self, string, doc_id):\n",
        "    self.string = string\n",
        "    self.doc_id = doc_id\n",
        "  \n",
        "  def __str__(self):\n",
        "    return f\"{self.string}: {self.doc_id}\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"{self.string}: {self.doc_id}\""
      ],
      "metadata": {
        "id": "P_X_2vKt8lbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StringWithDocIdAndPosition(StringWithDocId):\n",
        "  \"\"\"\n",
        "  Stores positions as well.\n",
        "  Used to create & sort list of positional terms.\n",
        "  \n",
        "  \"\"\"\n",
        "  def __init__(self, string, doc_id, position):\n",
        "    super().__init__(string, doc_id)\n",
        "    self.position = position\n",
        "  \n",
        "  def __lt__(token_with_doc_A, token_with_doc_B):\n",
        "    if token_with_doc_A.string != token_with_doc_B.string:\n",
        "      return token_with_doc_A.string < token_with_doc_B.string\n",
        "    elif token_with_doc_A.doc_id != token_with_doc_B.doc_id:\n",
        "      return token_with_doc_A.doc_id < token_with_doc_B.doc_id\n",
        "    else:\n",
        "      return token_with_doc_A.position < token_with_doc_B.position"
      ],
      "metadata": {
        "id": "XSBSe0-PxlCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Posting(UserDict):\n",
        "  \"\"\"\n",
        "  This stores the positional information for each term in the postional inverted index.\n",
        "  Some methods have been overriden to its objects behave like dictionary.\n",
        "  Keys are all of documents where this term appears;\n",
        "  Values store the positions, where in that document term appears\n",
        "  self.total_occurances stores the document_frequency\n",
        "  {docID: [pos1, pos2, ...]}.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.posting_dic = {}\n",
        "    self.total_occurances = 0\n",
        "  \n",
        "  def __contains__(self, doc_id):\n",
        "    return doc_id in self.posting_dic\n",
        "  \n",
        "  def __iter__(self):\n",
        "      return iter(self.posting_dic.items())\n",
        "\n",
        "  def __getitem__(self, doc_id):\n",
        "      return self.posting_dic[doc_id]\n",
        "    \n",
        "  def get(self, k, default=None):\n",
        "    return self[k]\n",
        "  \n",
        "  def __len__(self):\n",
        "      return len(self.posting_dic)\n",
        "\n",
        "  def add(self, doc_id, position):\n",
        "    if doc_id in self.posting_dic:\n",
        "      self.posting_dic[doc_id].append(position)\n",
        "    else:\n",
        "      self.posting_dic[doc_id] = [position]\n",
        "    self.total_occurances += 1\n",
        "  \n",
        "  def __str__(self):\n",
        "    return f\"{self.total_occurances} total occurances: {[f'{len(positions)} in {doc_id}: {positions}' for doc_id, positions in self.posting_dic.items()]}\"\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return str(self)"
      ],
      "metadata": {
        "id": "1Ts7G2B6TGRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this map is from https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n",
        "# I found it via https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
        "# removing contractions ensures more matches with stop-words and fixes the inconsitency of apostrohpes between stop-words and tokenizer\n",
        "# this will remove apostrophes and solve the issue of inconsistency between tokenization and stop-words\n",
        "def expandContractions(text):\n",
        "  \"\"\"\n",
        "  This function will iterate through the whole list of contractions\n",
        "  and replace all contracted forms in the given text with the full-length versions\n",
        "  \"\"\"\n",
        "  CONTRACTION_MAP = {\n",
        "  \"ain't\": \"is not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"i'd\": \"i would\",\n",
        "  \"i'd've\": \"i would have\",\n",
        "  \"i'll\": \"i will\",\n",
        "  \"i'll've\": \"i will have\",\n",
        "  \"i'm\": \"i am\",\n",
        "  \"i've\": \"i have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it would\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so as\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there would\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we would\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you would\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you will\",\n",
        "  \"you'll've\": \"you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "  }\n",
        "  for k, v in CONTRACTION_MAP.items():\n",
        "    text = text.replace(k, v)\n",
        "  return text"
      ],
      "metadata": {
        "id": "g-ML3mfxc8mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipediaPreprocessing(text):\n",
        "  # stop-words and regex only contain lower case versions of words\n",
        "  # therefore tokens must be lowercased in order to match them\n",
        "  # casefold() is a more aggresive version of lower()\n",
        "  # https://docs.python.org/3/library/stdtypes.html#str.casefold\n",
        "  text = text.casefold()\n",
        "  # reduce characters into ascii where meaning is preserved & seprate diacritics into individual characters\n",
        "  text = unicodedata.normalize('NFKD', text)\n",
        "  # remove all non ascii character i.e. separated diacritics\n",
        "  text = text.encode('ASCII', 'ignore').decode('UTF-8')\n",
        "  text = text.replace('[edit]', '')\n",
        "  text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "  # remove table of contents\n",
        "  text = re.sub(r'contents\\s+1\\s+plot.+plot', '', text)\n",
        "  # remove navigational hyperlinks\n",
        "  text = re.sub(r'episode chronology(.|\\n)+list of episodes', '', text)\n",
        "  text = re.sub(r'jump to (navigation|search)', '', text)\n",
        "  # remove other irrelevant wikipedia filler\n",
        "  text = text.replace('from wikipedia, the free encyclopedia', '')\n",
        "  text = text.replace('the simpsons episode', '')\n",
        "  # expand contractions\n",
        "  text =  expandContractions(text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "gpJRsN2bk5Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spelling correction!!\n",
        "From P. Norvig's website: https://norvig.com/spell-correct.html."
      ],
      "metadata": {
        "id": "ppA9N4C4LpmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellCorrect:\n",
        "\n",
        "  def __init__(self, posting_dict):\n",
        "    self.total_num_of_words = 0\n",
        "    self.word_freq_dic = {} \n",
        "    for key, val in posting_dict.items():\n",
        "      self.total_num_of_words += val.total_occurances\n",
        "      # I don't store positions here, because in case of spell check only frequencies are needed\n",
        "      self.word_freq_dic[key] = self.total_num_of_words\n",
        "\n",
        "  def _P(self, word): \n",
        "      \"Probability of `word`.\"\n",
        "      if word in self.word_freq_dic:\n",
        "        return self.word_freq_dic[word] / self.total_num_of_words\n",
        "      # If a word doesn't exist within the index\n",
        "      # It's probality is 0 - this is a way to get rid of words that are not real\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "  def correction(self, word): \n",
        "      \"Most probable spelling correction for word.\"\n",
        "      most_probable_correction = max(self._candidates(word), key=self._P)\n",
        "      # if no correction is possible just return None\n",
        "      if self._P(most_probable_correction) == 0:\n",
        "        return None\n",
        "      else:\n",
        "        return most_probable_correction\n",
        "\n",
        "  def _candidates(self, word): \n",
        "      \"Generate possible spelling corrections for word.\"\n",
        "      return (self._known([word]) or self._known(self._edits1(word)) or self._known(self._edits2(word)) or [word])\n",
        "\n",
        "  def _known(self, words): \n",
        "      \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "      return set(w for w in words if w in self.word_freq_dic.keys())\n",
        "\n",
        "  def _edits1(self, word):\n",
        "      \"All edits that are one edit away from `word`.\"\n",
        "      letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "      splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "      deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "      transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "      replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "      inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "      return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "  def _edits2(self, word): \n",
        "      \"All edits that are two edits away from `word`.\"\n",
        "      return (e2 for e1 in self._edits1(word) for e2 in self._edits1(e1))"
      ],
      "metadata": {
        "id": "qM1dEHFgNIiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiWordTermSet:\n",
        "  \"\"\"\n",
        "  Used to store multi-word terms;\n",
        "  when prompted finds all matching subsequences that match the stored terms\n",
        "  Currently used for multi-token episode titles, character and location names.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    # this will store terms in different sets based on the number of tokens each term has\n",
        "    # key [2] will have a set of processed terms of 2 tokens in length\n",
        "    self.dict_of_sets = dict()\n",
        "\n",
        "  def add(self, processed_tokens):\n",
        "    # anything of size 1, isn't a multi-word term - don't waste space storing it\n",
        "    if len(processed_tokens) > 1:\n",
        "      if len(processed_tokens) not in self.dict_of_sets:\n",
        "        # this is a set because there are some duplicates in the CSV files\n",
        "        self.dict_of_sets[len(processed_tokens)] = set()\n",
        "      self.dict_of_sets[len(processed_tokens)].add(\" \".join(processed_tokens))\n",
        "\n",
        "  def findMatchingSubsequences(self, list_of_processed_tokens: list) -> set:\n",
        "    \"\"\"\n",
        "    Go through list_of_processed_tokens and try to find matching subsequences \n",
        "    Return all found matches\n",
        "    \"\"\"\n",
        "    list_of_processed_tokens = list_of_processed_tokens[:max(self.dict_of_sets.keys())]\n",
        "    matching_subseqs = set()\n",
        "    for i in self.dict_of_sets.keys():\n",
        "      i_joined_tokens = \" \".join(list_of_processed_tokens[:i])\n",
        "      if i_joined_tokens in self.dict_of_sets[i]:\n",
        "        matching_subseqs.add(i_joined_tokens)\n",
        "    return matching_subseqs\n"
      ],
      "metadata": {
        "id": "5n5MJVlTbYlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Inverted Index (THE MOST IMPORTANT BIT)"
      ],
      "metadata": {
        "id": "U0qC9x-N40SG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WtSmBsrzqE7"
      },
      "outputs": [],
      "source": [
        "class InvertedIndex:\n",
        "    \"\"\"\n",
        "    Construct Inverted Index\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.inverted_index: dict = {}\n",
        "        self.csv_terms = MultiWordTermSet() \n",
        "        self.ep_titles = MultiWordTermSet() \n",
        "  \n",
        "    def _appendCSV(self, path: str) -> None:\n",
        "      \"\"\"\n",
        "      Used to fill up csv_terms that are later used to index multi-word terms within the corpus\n",
        "      1)opens a csv in the given path\n",
        "      2)processes each term, so that it matches the tokenized corpus\n",
        "      3)and appends to csv_terms set\"\"\"\n",
        "      df = pd.read_csv(path)\n",
        "      set_of_csv_entries = df['name']\n",
        "      for term in set_of_csv_entries:\n",
        "        # there is a bunch of numbered entries (~500) like \"Relative #1\" and \"Relative #2\"\n",
        "        # these should be considered to be the same entry,\n",
        "        # because characters aren't numbered like this in the corpus\n",
        "        term = re.sub(r'#\\d+', '', term)\n",
        "        processed_tokens = self.processListOfTokens(self.process_document(term))\n",
        "        self.csv_terms.add(processed_tokens)\n",
        "    \n",
        "    def _appendTitle(self, text):\n",
        "      \"\"\"This will extract the title (first line) from the given text, process it\n",
        "      lastly append it to list of known titles\"\"\"\n",
        "      unprocessed_ep_title = text.strip().split('\\n')[0]\n",
        "      processed_tokens = self.processListOfTokens(self.process_document(unprocessed_ep_title))\n",
        "      self.ep_titles.add(processed_tokens)\n",
        "\n",
        "    def read_data(self, path: str) -> list:\n",
        "        \"\"\"\n",
        "        Read files from a directory and then append the data of each file into a list.\n",
        "        \"\"\"\n",
        "        document_content_list = []\n",
        "        # go through all of the files in the given directory\n",
        "        print('read_data() has been called')\n",
        "        for file in tqdm(os.listdir(path)):\n",
        "          file_ext = os.path.splitext(file)[1].lower()\n",
        "          filename_with_path = os.path.join(path, file)\n",
        "          # if it's a txt file - append its contents to the output list\n",
        "          if file_ext == '.txt':\n",
        "            with open(filename_with_path, 'r') as f:\n",
        "              text = f.read()\n",
        "              document_content_list.append(StringWithDocId(text, file))\n",
        "              # extract the title and use it as a multi-token indexing\n",
        "              self._appendTitle(text)\n",
        "          # if it's a csv file - assume it's a character/location list and use it for multi-token indexing\n",
        "          elif file_ext == '.csv':\n",
        "            self._appendCSV(filename_with_path)\n",
        "          else:\n",
        "            print(filename_with_path, \" doesn't have the required file extension 'txt' or 'csv' - it will be skipped\")\n",
        "        print(\"Number of documents within the corpus:\", len(document_content_list)) \n",
        "        return document_content_list\n",
        "      \n",
        "    def processListOfTokens(self, tokenized: list) -> list:\n",
        "      \"\"\"\n",
        "      in a given list of tokens: removes stop-words, punctuation and stems the words.\n",
        "      \"\"\"\n",
        "      output = []\n",
        "      for token in tokenized:\n",
        "        # ignore tokens that don't have words/numbers in them i.e. punctuation only\n",
        "        if not re.search('\\w', token):\n",
        "          continue\n",
        "        # delete hyphens\n",
        "        token = token.replace('-', '')\n",
        "        # ignore stop-words\n",
        "        if token in STOP_WORDS:\n",
        "          continue\n",
        "        # apply stemming\n",
        "        stemmed = porter.stem(token)\n",
        "        output.append(stemmed)\n",
        "      return output\n",
        "\n",
        "    def process_document(self, document: str) -> list:\n",
        "        \"\"\"\n",
        "        pre-process a document and return a list of its terms\n",
        "        str->list\"\"\"\n",
        "        # Wikipedia hyperlinks should be removed\n",
        "        text = wikipediaPreprocessing(document)\n",
        "        tokenized = nltk.tokenize.word_tokenize(text)\n",
        "        # remove stop-words & apply stemming\n",
        "        filtered_tokens = self.processListOfTokens(tokenized)\n",
        "        return filtered_tokens\n",
        "\n",
        "    def _findMultiWordTerms(self, processed_token_subseq: list):\n",
        "      \"\"\"\n",
        "      This method tries to find all known multi-token subsections; starting from index [0] of the given processed_substring\n",
        "      \"\"\"        \n",
        "      csv_subseqs = self.csv_terms.findMatchingSubsequences(processed_token_subseq)\n",
        "      title_subseqs = self.ep_titles.findMatchingSubsequences(processed_token_subseq)\n",
        "      return csv_subseqs.union(title_subseqs)\n",
        "    \n",
        "    def index_corpus(self, documents: list) -> None:\n",
        "        \"\"\"\n",
        "        index given documents\n",
        "        list->None\"\"\"\n",
        "        starting_time = time.perf_counter()\n",
        "        token_list = []\n",
        "\n",
        "        # 1. Generate token list\n",
        "        print(\"1. Generate token list\")\n",
        "        for doc in tqdm(documents):\n",
        "          curr_doc_id = doc.doc_id\n",
        "          tokenized_doc = self.process_document(doc.string)\n",
        "          for i, token in enumerate(tokenized_doc):\n",
        "            matching_subseqs = self._findMultiWordTerms(tokenized_doc[i:])\n",
        "            # append all multi-word matches\n",
        "            for match in matching_subseqs:\n",
        "              match_with_doc_id_and_pos = StringWithDocIdAndPosition(match, curr_doc_id, i)\n",
        "              token_list.append(match_with_doc_id_and_pos)\n",
        "            # append current word\n",
        "            token_with_doc_id_and_pos = StringWithDocIdAndPosition(token, curr_doc_id, i)\n",
        "            token_list.append(token_with_doc_id_and_pos)\n",
        "        # 2. Sort\n",
        "        print(\"Sorting token list\")\n",
        "        sorted_token_list = sorted(token_list)\n",
        "        # 3. Convert into dictionary of postings\n",
        "        print(\"Creating a positional dictionary of postings\")\n",
        "        for i, token in enumerate(tqdm(sorted_token_list)):\n",
        "          if token.string not in self.inverted_index:\n",
        "            self.inverted_index[token.string] = Posting()\n",
        "          self.inverted_index[token.string].add(token.doc_id, token.position)\n",
        "        # Initialize spell-correct\n",
        "        self.spell_correct = SpellCorrect(self.inverted_index)\n",
        "        # Print out some details about the dataset\n",
        "        total_time_taken = round(time.perf_counter() - starting_time, 4)\n",
        "        print(f\"It took: {total_time_taken} seconds to index the whole corpus.\")\n",
        "        print(f\"It has {len(self.inverted_index)} entries in total.\")\n",
        "\n",
        "    def _processQuery(self, q: str) -> str:\n",
        "        query_as_list_of_processed_tokens = self.processListOfTokens(self.process_document(q))\n",
        "        # if query is a stop-word it will return None\n",
        "        if len(query_as_list_of_processed_tokens) == 0:\n",
        "          print(f\"Query '{q}' a stop-word\")\n",
        "          return None\n",
        "        # join a processed multi-word query\n",
        "        term = \" \".join(query_as_list_of_processed_tokens)\n",
        "        print(f\"{q} -> {term}\", end=\"; \")\n",
        "        if term in self.inverted_index:\n",
        "          return term\n",
        "        # attempt spell-correct\n",
        "        else:\n",
        "          most_probable_correction = self.spell_correct.correction(term)\n",
        "          if most_probable_correction == None:\n",
        "            print(\"No viable spelling fix was found\")\n",
        "          return most_probable_correction\n",
        "      \n",
        "    def dump(self, path: str) -> None:\n",
        "        \"\"\"\n",
        "        provide a dump function to show index entries for a given set of terms        \n",
        "        \"\"\"\n",
        "        print(\"passed_in_query -> query_after_processing: [list_of_occurances]\")\n",
        "        if os.path.exists(path) == False:\n",
        "          print(\"Path to file you provided doesn't exist\")\n",
        "          return\n",
        "        with open(path, 'r') as f:\n",
        "          file_contents = f.read()\n",
        "          examples = file_contents.split('\\n')\n",
        "          for query in examples:\n",
        "            processed_query = self._processQuery(query)\n",
        "            # if even after spell-correction no matching terms are found processed query will be None\n",
        "            if processed_query is not None:\n",
        "              print(processed_query, self.inverted_index[processed_query])\n",
        "     \n",
        "    def proximity_search(self, term1: str, term2: str, window_size: int = 3) -> dict:\n",
        "        \"\"\"\n",
        "        1) check whether given two terms appear within a window\n",
        "        2) calculate the number of their co-existance in a document\n",
        "        3) add the document id and the number of matches into a dict\n",
        "        return the dict\"\"\"\n",
        "        term1 = self._processQuery(term1)\n",
        "        if term1 is None:\n",
        "          print(\"Cannot find an entry in Inverted Index for\", term1)\n",
        "          return\n",
        "        term2 = self._processQuery(term2)\n",
        "        if term2 is None:\n",
        "          print(\"Cannot find an entry in Inverted Index for\", term2)\n",
        "          return\n",
        "        documents_containing_both_terms = {}\n",
        "        # i'm aware that this is usually implemented using `while` loops\n",
        "        # however in python I can do it using for loops\n",
        "        for term1_doc_id, term1_positions in self.inverted_index[term1]:\n",
        "          # if both terms can be found in the same document\n",
        "          if term1_doc_id in self.inverted_index[term2]:\n",
        "            # start iterating through positions of term1\n",
        "            for term1_position in term1_positions:\n",
        "              # for each position of term1 check distance of all positions of term2 \n",
        "              # (this loop can be broken early because the posting list sorted)\n",
        "              for term2_position in self.inverted_index[term2][term1_doc_id]:\n",
        "                if abs(term2_position - term1_position) <= window_size:\n",
        "                  if term1_doc_id not in documents_containing_both_terms:\n",
        "                    documents_containing_both_terms[term1_doc_id] = []\n",
        "                  documents_containing_both_terms[term1_doc_id].append((term1_position, term2_position))\n",
        "                # if term2 has passed the window of term1, move on to another term1 position\n",
        "                elif term2_position - term1_position > window_size:\n",
        "                  break\n",
        "        return documents_containing_both_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing my implementation"
      ],
      "metadata": {
        "id": "oFcrQZ0nJMeF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n4tuVF-zqE9"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"main call function\"\n",
        "    index = InvertedIndex() # initilaise the index\n",
        "    corpus = index.read_data('/content/drive/MyDrive/Colab Notebooks/Simpsons2022') # specify the directory path in which files are located\n",
        "    index.index_corpus(corpus) # index documents/corpus\n",
        "    return index\n",
        "index = main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test `dump` method using `.txt` provided on BlackBoard"
      ],
      "metadata": {
        "id": "_fvVChQ0l0NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index.dump(\"/content/drive/MyDrive/Colab Notebooks/26957722.txt\")"
      ],
      "metadata": {
        "id": "G94h70akKT-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print out most frequent words within the corpus"
      ],
      "metadata": {
        "id": "1tJzmR9xvFop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted([(k,v.total_occurances) for k, v in index.inverted_index.items()], key=lambda x: x[1], reverse=True)[:100]"
      ],
      "metadata": {
        "id": "gmH_ixZQtzma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test positional indexing"
      ],
      "metadata": {
        "id": "Kms5rGcflhG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# it's possible to find Gordie Howe using Boolean queries, even though he's not in the index\n",
        "index.proximity_search('Gordie', 'Howe', 1)"
      ],
      "metadata": {
        "id": "wld3Ax7Kk5K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diacritics will be converted to ascii and matched\n",
        "index.proximity_search('üter', 'character', 1)"
      ],
      "metadata": {
        "id": "c5f6wUg29uq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}